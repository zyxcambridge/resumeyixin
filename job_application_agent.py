#!/usr/bin/env python3
"""
è‡ªåŠ¨æ‰¾å·¥ä½œAgent - NVIDIAå²—ä½ç”³è¯·åŠ©æ‰‹
åŠŸèƒ½ï¼š
1. æœç´¢NVIDIAå²—ä½ï¼ˆAI Agentç›¸å…³ï¼Œä¸Šæµ·ï¼‰
2. è§£æå²—ä½è¦æ±‚
3. æ ¹æ®å²—ä½è¦æ±‚ç”Ÿæˆå®šåˆ¶åŒ–ç®€å†
4. æä¾›ç”³è¯·æŒ‡å¯¼
"""

import requests
from bs4 import BeautifulSoup
import json
import re
from typing import Dict, List, Optional
from pathlib import Path
import time

class NVIDIAJobAgent:
    def __init__(self, resume_path: str = "resume.tex"):
        self.resume_path = resume_path
        self.base_url = "https://nvidia.wd5.myworkdayjobs.com"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })
        
    def search_jobs(self, keywords: List[str] = ["AI Agent", "Multi-Agent"], 
                   location: str = "Shanghai, China", max_results: int = 20) -> List[Dict]:
        """
        æœç´¢NVIDIAå²—ä½
        """
        print(f"ğŸ” æœç´¢NVIDIAå²—ä½: {keywords}, åœ°ç‚¹: {location}")
        
        jobs = []
        search_url = f"{self.base_url}/en-US/NVIDIAExternalCareerSite/jobs"
        
        # è¿™é‡Œéœ€è¦å®é™…çš„æœç´¢é€»è¾‘
        # ç”±äºWorkdayç½‘ç«™éœ€è¦JavaScriptï¼Œæˆ‘ä»¬æä¾›æ‰‹åŠ¨æœç´¢æŒ‡å¯¼
        print("\nğŸ“‹ æœç´¢æŒ‡å¯¼ï¼š")
        print(f"1. è®¿é—®: {self.base_url}/en-US/NVIDIAExternalCareerSite")
        print(f"2. æœç´¢å…³é”®è¯: {' OR '.join(keywords)}")
        print(f"3. ç­›é€‰åœ°ç‚¹: {location}")
        print(f"4. å°†æ‰¾åˆ°çš„å²—ä½URLä¿å­˜åˆ° jobs.json æ–‡ä»¶ä¸­")
        
        return jobs
    
    def parse_job_description(self, job_url: str) -> Dict:
        """
        è§£æå²—ä½æè¿°å’Œè¦æ±‚
        """
        print(f"\nğŸ“– è§£æå²—ä½æè¿°: {job_url}")
        
        try:
            response = self.session.get(job_url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # æå–å²—ä½ä¿¡æ¯
            job_info = {
                'title': '',
                'location': '',
                'description': '',
                'requirements': [],
                'responsibilities': [],
                'url': job_url
            }
            
            # å°è¯•æå–æ ‡é¢˜
            title_elem = soup.find('h1') or soup.find('title')
            if title_elem:
                job_info['title'] = title_elem.get_text(strip=True)
            
            # æå–æè¿°å’Œè¦æ±‚
            # Workdayç½‘ç«™ç»“æ„å¤æ‚ï¼Œéœ€è¦æ ¹æ®å®é™…HTMLè°ƒæ•´
            desc_sections = soup.find_all(['div', 'section'], class_=re.compile(r'description|requirement|qualification', re.I))
            
            for section in desc_sections:
                text = section.get_text(strip=True)
                if 'requirement' in section.get('class', []) or 'qualification' in section.get('class', []):
                    job_info['requirements'].append(text)
                else:
                    job_info['description'] += text + "\n"
            
            return job_info
            
        except Exception as e:
            print(f"âŒ è§£æå¤±è´¥: {e}")
            print("ğŸ’¡ æç¤º: è¯·æ‰‹åŠ¨å¤åˆ¶å²—ä½æè¿°åˆ° job_description.txt")
            return {'url': job_url, 'title': 'éœ€è¦æ‰‹åŠ¨è¾“å…¥', 'requirements': []}
    
    def extract_keywords(self, job_description: str) -> List[str]:
        """
        ä»å²—ä½æè¿°ä¸­æå–å…³é”®è¯
        """
        # æŠ€æœ¯å…³é”®è¯
        tech_keywords = [
            'AI Agent', 'Multi-Agent', 'LLM', 'Large Language Model', 'NLP',
            'Deep Learning', 'Machine Learning', 'PyTorch', 'TensorFlow',
            'CUDA', 'GPU', 'Distributed Systems', 'Cloud Computing',
            'Autonomous Vehicle', 'Robotics', 'Computer Vision',
            'Reinforcement Learning', 'Transformer', 'Agent Framework'
        ]
        
        found_keywords = []
        desc_lower = job_description.lower()
        
        for keyword in tech_keywords:
            if keyword.lower() in desc_lower:
                found_keywords.append(keyword)
        
        return found_keywords
    
    def match_resume_sections(self, job_requirements: List[str], resume_content: str) -> Dict:
        """
        åŒ¹é…ç®€å†å†…å®¹ä¸å²—ä½è¦æ±‚
        """
        matches = {
            'relevant_experience': [],
            'relevant_skills': [],
            'relevant_projects': [],
            'match_score': 0
        }
        
        requirements_text = ' '.join(job_requirements).lower()
        
        # æ£€æŸ¥å·¥ä½œç»éªŒåŒ¹é…
        experience_keywords = self.extract_keywords(requirements_text)
        
        # ä»ç®€å†ä¸­æå–ç›¸å…³éƒ¨åˆ†
        # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…ç®€å†ç»“æ„è¿›è¡Œè§£æ
        
        return matches
    
    def generate_customized_resume(self, job_info: Dict, output_path: str) -> str:
        """
        æ ¹æ®å²—ä½è¦æ±‚ç”Ÿæˆå®šåˆ¶åŒ–ç®€å†
        """
        print(f"\nâœï¸  ç”Ÿæˆå®šåˆ¶åŒ–ç®€å†: {output_path}")
        
        # è¯»å–åŸå§‹ç®€å†
        with open(self.resume_path, 'r', encoding='utf-8') as f:
            resume_content = f.read()
        
        # æå–å²—ä½å…³é”®è¯
        all_requirements = ' '.join(job_info.get('requirements', []))
        keywords = self.extract_keywords(all_requirements)
        
        print(f"ğŸ“Œ å²—ä½å…³é”®è¯: {', '.join(keywords)}")
        
        # ç”Ÿæˆå®šåˆ¶åŒ–ç‰ˆæœ¬
        customized_resume = resume_content
        
        # åœ¨ç®€å†å¼€å¤´æ·»åŠ å²—ä½åŒ¹é…è¯´æ˜ï¼ˆæ³¨é‡Šå½¢å¼ï¼‰
        job_match_comment = f"""
% ============================================
% å®šåˆ¶åŒ–ç®€å† - é’ˆå¯¹å²—ä½: {job_info.get('title', 'NVIDIA Position')}
% å²—ä½URL: {job_info.get('url', '')}
% åŒ¹é…å…³é”®è¯: {', '.join(keywords)}
% ç”Ÿæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}
% ============================================
"""
        customized_resume = job_match_comment + customized_resume
        
        # ä¿å­˜å®šåˆ¶åŒ–ç®€å†
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(customized_resume)
        
        print(f"âœ… å®šåˆ¶åŒ–ç®€å†å·²ä¿å­˜: {output_path}")
        
        return output_path
    
    def create_application_guide(self, job_info: Dict, resume_path: str) -> str:
        """
        åˆ›å»ºç”³è¯·æŒ‡å¯¼æ–‡æ¡£
        """
        guide = f"""
# NVIDIAå²—ä½ç”³è¯·æŒ‡å¯¼

## å²—ä½ä¿¡æ¯
- **èŒä½**: {job_info.get('title', 'N/A')}
- **URL**: {job_info.get('url', 'N/A')}
- **åœ°ç‚¹**: {job_info.get('location', 'Shanghai, China')}

## ç”³è¯·æ­¥éª¤

### 1. å‡†å¤‡ææ–™
- âœ… å®šåˆ¶åŒ–ç®€å†: {resume_path}
- âœ… è‹±æ–‡ç®€å†: resume_en.pdf
- âœ… æ±‚èŒä¿¡ï¼ˆå¯é€‰ï¼‰

### 2. ç”³è¯·æµç¨‹
1. è®¿é—®å²—ä½URL: {job_info.get('url', '')}
2. ç‚¹å‡» "Apply" æŒ‰é’®
3. å¡«å†™ä¸ªäººä¿¡æ¯
4. ä¸Šä¼ ç®€å†: {resume_path.replace('.tex', '.pdf')}
5. å›ç­”ç”³è¯·é—®é¢˜
6. æäº¤ç”³è¯·

### 3. ç®€å†å®šåˆ¶è¦ç‚¹
- å¼ºè°ƒä¸å²—ä½ç›¸å…³çš„é¡¹ç›®ç»éªŒ
- çªå‡ºåŒ¹é…çš„æŠ€æœ¯æ ˆ
- é‡åŒ–æˆæœå’Œå½±å“

### 4. æ³¨æ„äº‹é¡¹
- âš ï¸ ç¡®ä¿ä¿¡æ¯çœŸå®ï¼Œä¸è¦é€ å‡
- âš ï¸ æ¯ä¸ªå²—ä½ä½¿ç”¨å®šåˆ¶åŒ–ç®€å†
- âš ï¸ ä»”ç»†é˜…è¯»å²—ä½è¦æ±‚
- âš ï¸ ä¿å­˜ç”³è¯·ç¡®è®¤ä¿¡æ¯

## å²—ä½è¦æ±‚æ‘˜è¦
{chr(10).join(f'- {req[:200]}...' if len(req) > 200 else f'- {req}' for req in job_info.get('requirements', [])[:5])}

---
ç”Ÿæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}
"""
        
        guide_path = f"application_guide_{job_info.get('title', 'job').replace(' ', '_')[:30]}.md"
        with open(guide_path, 'w', encoding='utf-8') as f:
            f.write(guide)
        
        print(f"ğŸ“ ç”³è¯·æŒ‡å¯¼å·²ä¿å­˜: {guide_path}")
        return guide_path


def main():
    """
    ä¸»å‡½æ•° - è‡ªåŠ¨åŒ–ç”³è¯·æµç¨‹
    """
    print("=" * 60)
    print("ğŸš€ NVIDIAå²—ä½ç”³è¯·åŠ©æ‰‹")
    print("=" * 60)
    
    agent = NVIDIAJobAgent()
    
    # ç¤ºä¾‹ï¼šå¤„ç†ç‰¹å®šå²—ä½
    job_urls = [
        "https://nvidia.wd5.myworkdayjobs.com/en-US/NVIDIAExternalCareerSite/details/Senior-Software-Engineer--Multi-Agent-System---AV-Infrastructure_JR2010348",
        "https://nvidia.wd5.myworkdayjobs.com/en-US/NVIDIAExternalCareerSite/details/Developer-Technology-Engineer--AI_JR2000017"
    ]
    
    print("\nğŸ“‹ å¤„ç†å²—ä½åˆ—è¡¨:")
    for i, url in enumerate(job_urls, 1):
        print(f"{i}. {url}")
    
    # å¤„ç†æ¯ä¸ªå²—ä½
    for i, job_url in enumerate(job_urls, 1):
        print(f"\n{'='*60}")
        print(f"å¤„ç†å²—ä½ {i}/{len(job_urls)}")
        print(f"{'='*60}")
        
        # è§£æå²—ä½æè¿°
        job_info = agent.parse_job_description(job_url)
        
        # ç”Ÿæˆå®šåˆ¶åŒ–ç®€å†
        job_title_safe = re.sub(r'[^\w\s-]', '', job_info.get('title', f'job_{i}'))[:50]
        resume_output = f"resume_customized_{job_title_safe.replace(' ', '_')}.tex"
        
        agent.generate_customized_resume(job_info, resume_output)
        
        # ç¼–è¯‘PDF
        print(f"\nğŸ“„ ç¼–è¯‘PDF...")
        import subprocess
        try:
            subprocess.run(['xelatex', '-interaction=nonstopmode', resume_output], 
                         check=True, capture_output=True)
            print(f"âœ… PDFå·²ç”Ÿæˆ: {resume_output.replace('.tex', '.pdf')}")
        except Exception as e:
            print(f"âš ï¸  PDFç¼–è¯‘å¤±è´¥: {e}")
            print("ğŸ’¡ è¯·æ‰‹åŠ¨è¿è¡Œ: xelatex resume_customized_*.tex")
        
        # åˆ›å»ºç”³è¯·æŒ‡å¯¼
        agent.create_application_guide(job_info, resume_output)
        
        print(f"\nâœ… å²—ä½ {i} å¤„ç†å®Œæˆ!")
        time.sleep(2)  # é¿å…è¯·æ±‚è¿‡å¿«
    
    print("\n" + "=" * 60)
    print("ğŸ‰ æ‰€æœ‰å²—ä½å¤„ç†å®Œæˆ!")
    print("=" * 60)
    print("\nğŸ“Œ ä¸‹ä¸€æ­¥:")
    print("1. æ£€æŸ¥ç”Ÿæˆçš„å®šåˆ¶åŒ–ç®€å†")
    print("2. è®¿é—®å²—ä½URLè¿›è¡Œç”³è¯·")
    print("3. ä¸Šä¼ å¯¹åº”çš„å®šåˆ¶åŒ–ç®€å†PDF")
    print("4. ä¿å­˜ç”³è¯·ç¡®è®¤ä¿¡æ¯")


if __name__ == "__main__":
    main()
